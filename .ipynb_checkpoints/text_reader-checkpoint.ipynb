{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "bot_reply = pd.read_csv('bot_reply.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot_reply\n",
    "type(bot_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bot_reply.reset_index()\n",
    "#list(bot_reply.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ['when', 'will', 'my', 'card', 'arrive'] \n",
    "document = 'when will my card be arriving?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['card', 'arriv']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "import os,sys\n",
    "\n",
    "#-----------------------#\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def text_prepare(document):\n",
    "    #remove punctuation\n",
    "    punctuation = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(punctuation) for w in document]\n",
    "\n",
    "    #tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized_list = (tokenizer.tokenize(document))\n",
    "\n",
    "    #remove stopwords\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "\n",
    "    wordsFiltered = []\n",
    "    for w in tokenized_list:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "\n",
    "    #stemming\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    wordsFiltered2 = []\n",
    "    for word in wordsFiltered:\n",
    "        wordsFiltered2.append(ps.stem(word))\n",
    "\n",
    "    return wordsFiltered2\n",
    "\n",
    "text_prepare(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['card', 'arriv']\n",
      "['card', 'arriv']\n",
      "Your card should take one to two weeks to arrive from the date of application.\n",
      "['get', 'card']\n",
      "['card', 'arriv']\n"
     ]
    }
   ],
   "source": [
    "bot_reply_dict = bot_reply.set_index('words').T.to_dict()\n",
    "\n",
    "for key, value in bot_reply_dict.items():\n",
    "    key = text_prepare(key)\n",
    "    print(key)\n",
    "    print(text_prepare(document))\n",
    "    if(set(text_prepare(document)) == set(key)):\n",
    "        print(value['reply'])\n",
    "        reply = value['reply']\n",
    "#             break\n",
    "#         else: pass\n",
    "#bot_reply_dict\n",
    "#bot_reply_dict['when, card, arrive']['reply']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-ec2578d30450>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmooth_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtfidf_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_count_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf_transformer' is not defined"
     ]
    }
   ],
   "source": [
    "#using Tfid Vectorizer to score the similarity of the 2\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#instantiate CountVectorizer()\n",
    "cv = CountVectorizer()\n",
    " \n",
    "# this steps generates word counts for the words in your docs\n",
    "word_count_vector = cv.fit_transform(text_prepare(document))\n",
    "word_count_vector.shape\n",
    "\n",
    "tfidf_matrix=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_fit = tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "\n",
    "# print idf values\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    " \n",
    "# sort ascending\n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-6489667a0cd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mget_recommendations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbot_reply\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcosine_sim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0msim_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_sim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'indices' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#from config import onetime_file\n",
    "# from nlp.nlp_solutions.nlplearn import *\n",
    "# import os,string,sys\n",
    "# sys.path.append(os.path.normpath(os.getcwd()))\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "#from config import follow_up_path, location\n",
    "#from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "#cosine similarity search\n",
    "# def tfidf_fit(docs):\n",
    "    \n",
    "#     docs = [text_prepare(text) for text in docs]\n",
    "#     tfidf_vectorizer = TfidfVectorizer()\n",
    "#     tfidf_fit = tfidf_vectorizer.fit(docs)\n",
    "#     tfidf_matrix = tfidf_fit.transform(docs)\n",
    "    \n",
    "#     return tfidf_fit, tfidf_matrix\n",
    "\n",
    "#match query with bot_reply answer\n",
    "def similarity_search(doc, list_index, tfidf_fit=tfidf_fit,tfidf_matrix=tfidf_matrix):\n",
    "    out = cosine_similarity(tfidf_fit.transform([text_prepare(doc)]), tfidf_matrix.tocsr()[list_index,:])\n",
    "    a = list(out[0])\n",
    "    b = sorted(range(len(a)), key=lambda i: a[i], reverse=True)[0]\n",
    "    return b\n",
    "\n",
    "\n",
    "def bot_filtering(docs):\n",
    "    docs = docs.fillna('')\n",
    "    tfidf_matrix = tfidf.fit_transform(docs)\n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    return cosine_sim\n",
    "\n",
    "def get_recommendations(reply, data=bot_reply,indices=indices, cosine_sim=cosine_sim):\n",
    "    idx = indices[reply]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:4]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    return metadata['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-fea6dfbc5e8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbot_filtering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbot_reply\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbot_reply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbot_reply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reply'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-6489667a0cd2>\u001b[0m in \u001b[0;36mbot_filtering\u001b[0;34m(docs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbot_filtering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mtfidf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "cosine_sim = bot_filtering(bot_reply)\n",
    "indices = pd.Series(bot_reply.index, index=bot_reply['reply'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = similarity_search(search_term, list(metadata.index))\n",
    "question, reply = metadata.loc[b]['words'], metadata.loc[b]['reply']\n",
    "message_buttons(channel, results, links, 'Showing results for \"' + str(search_term)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
